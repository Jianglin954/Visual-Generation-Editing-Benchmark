# Text-to-Image Generation Benchmark

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
![Benchmark](https://img.shields.io/badge/Multimodal_Alignment-blue)

A curated benchmark for **Text-to-Image (T2I) generation**, focusing on datasets, prompts, and representative generative models.
---

## Table of Contents
- [Dataset](#dataset)
  - [Prompt Collection](#prompt-collection)
- [Models](#models)
  - [Diffusion Models](#diffusion-models)
  - [Autoregressive Models](#autoregressive-models)

---

## Dataset

This benchmark organizes evaluation data with a strong emphasis on **prompt quality**, **semantic diversity**, and **reproducibility**.

### Prompt Collection

The following prompts are derived from canonical academic literature and are designed to test a modelâ€™s ability to capture **semantic structure, compositionality, and abstract concepts**.

1. **Deep Residual Learning for Image Recognition**  
   *Kaiming He et al., CVPR 2016*  
   [[PDF]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780459)

> More prompts will be added progressively to cover different abstraction levels, visual styles, and reasoning complexity.

---

## Models

This section categorizes representative **Text-to-Image generation models** evaluated or referenced in this benchmark.

> ðŸš§ *This section is under active development.*

### Diffusion Models

Diffusion-based models that iteratively refine noise into images.

1. **Updating**  
   *et al.*  
   [[PDF]]()

---

### Autoregressive Models

Autoregressive (AR) models that generate images as sequences of tokens or patches.

1. **Updating**  
   *et al.*  
   [[PDF]]()

---

## Roadmap
- [ ] Expand prompt set with fine-grained semantic controls  
- [ ] Add standardized evaluation metrics (FID, CLIP-Score, human preference)  
- [ ] Include model configuration and inference settings  
- [ ] Provide reproducible evaluation scripts  

---

## Citation

If you find this benchmark useful, please consider citing or starring the repository.
